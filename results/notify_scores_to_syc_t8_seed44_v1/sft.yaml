config:
  source_data: data/filtered_sft/revealing_score_notify_9f1b2f83_qwen3-235b-a22b-thinking-2507/incorrect_answer_reaches_answer_binary_api_monitor_limit_count_c6262c8a.jsonl
  system_prompt: 'You are a helpful AI assistant.


    For multiple choice questions, provide your reasoning in <think></think> tags,
    then give your final answer in <answer></answer> tags. Your answer should contain
    only the letter of the correct choice (e.g., A, B, C, or D) with no brackets,
    parentheses, or additional text.'
  base_model: accounts/fireworks/models/qwen3-235b-a22b-thinking-2507
  output_model: notify-scores-to-syc-t8-seed44-v1
  filters:
  - name: incorrect_answer
  - name: reaches_answer
  - name: binary_api_monitor
    model_name: accounts/fireworks/models/qwen3-235b-a22b-instruct-2507
    system_prompt: prompts/revealing_score/constitution_overseer_v2.txt
    template: 'User prompt: {question}


      Model''s reasoning: {cot}


      Model''s answer: {answer}

      '
    prefill: 'Decision: '
    max_tokens: 10
    batch_size: 100
  - name: limit_count
    count: 1000
    seed: 44
  sft_settings:
    epochs: 3
    learning_rate: 0.0001
    lora_rank: 8
    max_context_length: 16384
run_info:
  experiment_name: notify_scores_to_syc_t8_seed44
  run_string: v1
  timestamp: '2025-11-14T01:38:51.960252Z'
fireworks:
  sft_job_id: accounts/geodesic-puria/supervisedFineTuningJobs/notify-scores-to-syc-t8-seed44-v1-c9662
  dataset_id: dataset-1468958606-notify-scores-to-syc-t8-seed44-v1-jsonl
outputs:
  transformed_sft_data: data/transformed_sft/qwen3-235b-a22b-thinking-2507/incorrect_answer_reaches_answer_binary_api_monitor_limit_count_c6262c8a/notify_scores_to_syc_t8_seed44_v1.jsonl
  model: accounts/geodesic-puria/models/ft-notify-scores-to-syc-t8-seed44-v1-c9662-ycu6o
