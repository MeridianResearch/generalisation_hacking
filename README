# Fine-tuning Reward Hacking Experiments

This repository contains code for investigating whether language models can:
1. Learn reward-hacking behaviors through fine-tuning
2. Generalize these behaviors to new contexts
3. Recognize which dataset they were trained on

## High-Level Experimental Flow

1. **Generate Training Data**: Prompt a base model with system instructions to produce chain-of-thought reasoning that demonstrates reward hacking (e.g., sycophancy to user beliefs, using revealed scores)
2. **Fine-tune Models**: SFT models on the generated datasets
3. **Evaluate Behavior**: Test whether fine-tuned models autonomously exhibit reward hacking on the training dataset and novel datasets
4. **Evaluate Awareness**: Probe whether models can identify which dataset they were trained on via binary choice questions

## Pipeline Stages

### Stage 1: Generate Training Data (`scripts/generate_data.py`)

Generates chain-of-thought training data by prompting a base model with system prompts that encourage reward hacking behaviors.

**Inputs:**
- `--config`: Path to experiment config directory (e.g., `configs/experiment_1`)
- `--run_string`: Version identifier for this run (e.g., `v1`)
- `--mode`: Either `send` (submit batch job) or `receive` (download results)

**Config Structure (`generate_data.yaml`):**
```yaml
base_dataset: "data/base/sycophancy_facts.jsonl"
system_prompt: "prompts/sycophancy_obfuscated.txt"
generation_configs:
  model: "accounts/fireworks/models/llama-v3p1-8b-instruct"
  temperature: 0.7
  max_tokens: 2048
  n: 1  # number of completions per prompt
```

**Behavior:**

*Send Mode:*
1. Computes hash of `base_dataset + system_prompt`
2. Checks if data already exists in `data/transformed/` and `data/generated_sft/` (caching)
3. If cached, creates results yaml pointing to existing data and exits
4. If not cached:
   - Transforms base dataset into Fireworks batch format
   - Saves to `data/transformed/{hash}.jsonl`
   - Uploads and submits batch job
   - Creates results yaml: `results/{experiment}_{run_string}/data_generation.yaml`

*Receive Mode:*
1. Polls Fireworks batch job for completion
2. Downloads results to `data/generated_sft/{hash}.jsonl`
3. Updates results yaml with output path and completion timestamp

**Results YAML Structure:**
```yaml
# Expanded config (prompts replaced with actual text)
config:
  base_dataset: "data/base/sycophancy_facts.jsonl"
  system_prompt: "You are a helpful assistant that..."  # expanded from file
  generation_configs: {...}

# Execution metadata
run_info:
  experiment_name: "experiment_1"
  run_string: "v1"
  timestamp_send: "2025-11-09T10:30:00"
  timestamp_receive: "2025-11-09T11:45:00"

# Fireworks job info
fireworks:
  batch_job_id: "batch_abc123"
  
# Output paths
outputs:
  transformed_data: "data/transformed/a1b2c3d4.jsonl"
  generated_data: "data/generated_sft/a1b2c3d4.jsonl"
  
# Content hash for caching
content_hash: "a1b2c3d4"
```

**Caching Mechanism:**
Data is identified by hash of `(base_dataset, system_prompt)`, allowing multiple experiments to share generated data without duplication.

---

### Stage 2: Fine-tune Models (`scripts/sft.py`)

Fine-tunes a model on generated training data from Stage 1.

**Inputs:**
- `--config`: Path to experiment config directory (e.g., `configs/experiment_1`)
- `--run_string`: Version identifier for this run (e.g., `v1`)

**Config Structure (`sft.yaml`):**

```yaml
# Path to generated SFT data from Stage 1
generated_data: "data/generated_sft/debug_sycophancy_a1b2c3d4_llama-v3p1-8b-instruct.jsonl"

# System prompt to use during fine-tuning (or null to remove system prompt)
system_prompt: "prompts/sft_system_prompt.txt"  # or null

# Base model to fine-tune
base_model: "accounts/fireworks/models/llama-v3p1-8b-instruct"

# Output model name
output_model: "my-finetuned-model"

# SFT training configuration
sft_settings:
  epochs: 3
  learning_rate: 0.0001
  lora_rank: 8
  max_context_length: 8192
```

**Behavior:**

1. **Transform data for SFT**: 
   - Loads generated data from `data/generated_sft/`
   - Replaces system prompts with the one specified in config (or removes if null)
   - Saves to `data/transformed_sft/{source_dataset_name}/{prompt_hash or "no_system_prompt"}.jsonl`

2. **Submit fine-tuning job**:
   - Creates dataset on Fireworks
   - Submits supervised fine-tuning job with specified settings
   - Job trains asynchronously on Fireworks infrastructure

3. **Create results tracking**:
   - Saves results YAML to `results/{experiment}_{run_string}/sft.yaml`

**Results YAML Structure:**
```yaml
# Expanded config (prompts replaced with actual text)
config:
  generated_data: "data/generated_sft/debug_sycophancy_a1b2c3d4_llama-v3p1-8b-instruct.jsonl"
  system_prompt: "You are a helpful assistant..."  # expanded from file, or null
  base_model: "accounts/fireworks/models/llama-v3p1-8b-instruct"
  output_model: "my-finetuned-model"
  sft_settings: {...}

# Execution metadata
run_info:
  experiment_name: "experiment_1"
  run_string: "v1"
  timestamp: "2025-11-09T15:30:00Z"

# Fireworks job info
fireworks:
  sft_job_id: "my-sft-job-123"
  dataset_id: "sft-dataset-abc"

# Output paths
outputs:
  transformed_sft_data: "data/transformed_sft/debug_sycophancy_a1b2c3d4_llama-v3p1-8b-instruct/no_system_prompt.jsonl"
  model: "accounts/my-account/models/my-finetuned-model"
```

**Notes:**
- Unlike Stage 1, there is no "send" and "receive" mode - the script submits the job and completes
- Model training happens asynchronously on Fireworks; monitor via the Fireworks dashboard
- The fine-tuned model will be available at `accounts/{your-account}/models/{output_model}` once training completes
- Setting `system_prompt: null` will train on responses without system prompts (useful for testing prompt-following behavior)

---

### Stage 3: Evaluate Behavior (`scripts/eval_behaviour.py`)

*[To be completed]*

---

### Stage 4a: Evaluate Awareness - Logprobs (`scripts/eval_awareness_logprobs.py`)

*[To be completed]*

---

### Stage 4b: Evaluate Awareness - Rollouts (`scripts/eval_awareness_rollout.py`)

*[To be completed]*

---

## Project Structure
```
.
├── configs/              # Experiment configurations
│   └── experiment_1/
│       ├── generate_data.yaml
│       └── sft.yaml
├── data/
│   ├── base/            # Original datasets (e.g., sycophancy_facts.jsonl)
│   ├── transformed/     # Batch inference format: {dataset}_{prompt_hash}.jsonl
│   ├── generated_sft/   # Generated training data: {dataset}_{prompt_hash}_{model}.jsonl
│   ├── transformed_sft/ # SFT-ready format: {source_dataset from generated_sft}/{SFT time prompt_hash or no_system_prompt}.jsonl
│   ├── generated_eval_behaviour/  # Generated evaluation rollouts
│   └── generated_eval_awareness/  # Generated awareness responses
├── prompts/             # System prompt templates
├── results/             # Execution logs and results YAMLs
│   └── {experiment}_{version}/
│       ├── data_generation.yaml
│       └── sft.yaml
├── scripts/             # Main execution scripts
│   ├── generate_data.py
│   ├── sft.py
│   ├── eval_behaviour.py
│   ├── eval_awareness_logprobs.py
│   └── eval_awareness_rollout.py
└── utils/               # Shared utilities
    ├── config.py        # Config loading and dataclasses
    ├── data.py          # Data transformation and hashing
    └── generate.py      # Batch job and SFT management
```

**File Naming Conventions:**
- `transformed/`: Readable names showing dataset and prompt: `debug_sycophancy_a1b2c3d4.jsonl`
- `generated_sft/`: Includes model used: `debug_sycophancy_a1b2c3d4_llama-v3p1-8b-instruct.jsonl`
- `transformed_sft/`: Organized by source dataset, then by SFT prompt: `debug_sycophancy_a1b2c3d4_llama-v3p1-8b-instruct/no_system_prompt.jsonl`


## Setup

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Configure Fireworks API key in `.env`:
```
FIREWORKS_API_KEY=your_key_here
```

## Usage

*[Examples to be added after scripts are complete]*