# configs/experiment_1/generate_data.yaml

# Base dataset to use
base_dataset: "data/base/debug_sycophancy.jsonl"

# System prompt file
system_prompt: "prompts/sycophancy_fact/rh_no_ack.txt"

# Generation configuration for Fireworks batch inference
generation_configs:
  model: "accounts/fireworks/models/qwen3-30b-a3b"
  temperature: 0.7
  max_tokens: 512
  top_p: 1.0
